{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load Recipes Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "recipes = json.load(open('./data/recipe_train.json'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convert recipes to text documents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "recipes_as_doc = {}\n",
    "\n",
    "for sample in recipes:\n",
    "    key = sample['cuisine']\n",
    "    # If key is in the dictionary, return its value. If not, insert key with a value of default and return default.\n",
    "    recipes_as_doc.setdefault(key,[]).append(' '.join(sample['ingredients']).lower())\n",
    "\n",
    "# create a single list with all the documents\n",
    "all_docs = []\n",
    "for k, v in recipes_as_doc.items():\n",
    "   all_docs.append(' '.join(v))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tf-idf vectorizer for text data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# code adapted from https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "transformed_documents = vectorizer.fit_transform(all_docs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Analyze results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "greek southern_us filipino indian jamaican spanish italian mexican chinese british thai vietnamese cajun_creole brazilian french japanese irish korean moroccan russian "
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "docs_scores_dfs = {}\n",
    "\n",
    "# loop each item in transformed_documents_as_array, using enumerate to keep track of the current position\n",
    "for counter, doc in enumerate(transformed_documents_as_array):\n",
    "    # construct a data frame\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names(), doc))\n",
    "    one_doc_as_df = pd.DataFrame\\\n",
    "        .from_records(tf_idf_tuples, columns=['term', 'score'])\\\n",
    "        .sort_values(by='score', ascending=False)\\\n",
    "        .reset_index(drop=True)\n",
    "    docs_scores_dfs[list(recipes_as_doc.keys())[counter]] = one_doc_as_df\n",
    "\n",
    "    print(list(recipes_as_doc.keys())[counter], end=' ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Statistics of Scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "statistics = {}\n",
    "for k in docs_scores_dfs.keys():\n",
    "    df = docs_scores_dfs[k]\n",
    "    scores = df['score']\n",
    "    scores = scores.loc[scores>0.0]\n",
    "    statistics[k] = [scores.mean(), scores.max(), scores.min(), df['score'].astype(bool).sum(axis=0)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load Baskets Market Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# ! this parameter will be also used for outputting basketScore and clustering\n",
    "basketsFilename= \"synthetic20000.csv\"\n",
    "\n",
    "baskets = []\n",
    "with open('./data/' + basketsFilename, newline=None) as f:\n",
    "  reader = csv.reader(f)\n",
    "  for row in reader:\n",
    "    baskets.append([elem.replace(' ', '-') for elem in row])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Optimize computation\n",
    "In order to optimize computation we compute ngram for tf.idf top score \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "top_scores_ngram = {}\n",
    "CUISINE_TOP_SCORE_THRESHOLD = 0.1\n",
    "for cuisine in docs_scores_dfs.keys():\n",
    "    scores_df = docs_scores_dfs[cuisine]\n",
    "    scores_df = scores_df[scores_df['score']>CUISINE_TOP_SCORE_THRESHOLD]\n",
    "    scores_df = scores_df.sort_values(by=['score'],ascending=False)\n",
    "    \n",
    "    ngram_scores = []\n",
    "    for index, row in scores_df.iterrows():\n",
    "        ngram = list(ngrams(row['term'], 3))\n",
    "        score =  row['score']\n",
    "        ngram_scores.append([ngram, score, row['term']])\n",
    "        \n",
    "    top_scores_ngram[cuisine] = ngram_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define the distance metric "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def jaccard_distance(a, b):\n",
    "    \"\"\"Calculate the jaccard distance between sets A and B\"\"\"\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    return 1.0 * len(a&b)/len(a|b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Optimize computation\n",
    "In order to optimize computation we calculate the ngrams of unique items in the baskets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "items_ngrams = {}\n",
    "for basket in baskets:\n",
    "    for item in basket:\n",
    "        if items_ngrams.get(item) is None :\n",
    "            items_ngrams[item] = list(ngrams(item, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compute Basket Scores\n",
    "For each basket we iterate over all items and compute their similarities with the top \n",
    "terms in the cuisines docs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "--- 20.766939640045166 seconds ---\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "baskets_scores = []\n",
    "all_similarities = {}\n",
    "for basket in baskets:\n",
    "    similarities = {}\n",
    "    for cuisine in top_scores_ngram:\n",
    "        value = 0\n",
    "        sims = 0\n",
    "        for item in basket:      \n",
    "            item_ngram = items_ngrams[item]\n",
    "            for ngram_term, score, term in top_scores_ngram[cuisine]:\n",
    "                if all_similarities.get(item+term) is None:\n",
    "                    if all_similarities.get(term+item) is None:\n",
    "                        sim = jaccard_distance(item_ngram, ngram_term)\n",
    "                        all_similarities[item+term] = sim\n",
    "                        all_similarities[term+item] = sim\n",
    "                    else:\n",
    "                        sim = all_similarities[term+item]\n",
    "                else:\n",
    "                    sim = all_similarities[item+term]\n",
    "\n",
    "                if sim > 0.2:\n",
    "                    value += sim * score\n",
    "                    sims += sim\n",
    "        if sims != 0:\n",
    "            value = value/sims\n",
    "        else:\n",
    "            value=0\n",
    "                    \n",
    "        similarities[cuisine] = value    \n",
    "    baskets_scores.append(similarities)\n",
    "    \n",
    "baskets_scores_df = pd.DataFrame(baskets_scores)\n",
    "baskets_scores_df=(baskets_scores_df-baskets_scores_df.mean())/baskets_scores_df.std()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# save this data if you want to run elki clustering from dbscanELKI.sh script\n",
    "baskets_scores_df.to_csv('./data/scores_' + basketsFilename, header=False, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Clustering baskets usign DBSCAN in ELKI Data Mining Framework\n",
    "\n",
    "This is done calling the script present in the folder. Once the script completes the next cell \n",
    "will output 0.  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 19
    }
   ],
   "source": [
    "# if necessary change parameters and paths\n",
    "os.system(\"./dbscanELKI.sh \\\n",
    "    eps=0.01 minPoints=1000 \\\n",
    "    data=/home/vale/Documenti/Uni/II-I\\ Sem/Data\\ Mining/Project/data/scores_\" + basketsFilename + \" \\\n",
    "    log=/home/vale/Documenti/Uni/II-I\\ Sem/Data\\ Mining/Project/data/log_\" + basketsFilename + \" \\\n",
    "    output=/home/vale/Documenti/Uni/II-I\\ Sem/Data\\ Mining/Project/data/clusters__\" + basketsFilename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-4979b13c",
   "language": "python",
   "display_name": "PyCharm (Project)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}